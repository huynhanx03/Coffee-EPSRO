# -*- coding: utf-8 -*-
"""Coffee Question Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10i_1LdmEy0saf7AAOtsh71mcBTVSbzV6
"""

from google.colab import drive
drive.mount('/content/drive')

"""# 0.1. Create data"""

begin0s = [
    "", "xin chào", "chào"
]

begin1s = [
  "", "epsro", "cửa hàng", "nhân viên", "bạn", "cậu", "em", "anh"
]

begin2s = [
  "", "cho anh hỏi", "cho em hỏi", "cho mình hỏi", "cho tớ hỏi", "cho tôi hỏi"
]

separas = [
    " ",
    # ", "
]

ends = [
    "",
    # "."
]

objects = [
    "tôi", "anh", "em", "mình", "tớ"
]

productNames = ['cà phê']

# Câu hỏi dựa trên tiêu chí:
# + Số lượng
# + Thông tin ()
# + Giá
# + Mô tả
# + Công thức

productInformationQuestions = [
    'thông tin về "sản phẩm"',

    # Số lượng
    '"sản phẩm" còn hàng không',
    'hiện tại có thể có bao nhiêu "sản phẩm"',
    # 'hiện tại có thể đặt bao nhiêu "sản phẩm"',

    # Giá tiền
    # '"sản phẩm" bao nhiêu tiền',
    '"sản phẩm" giá bao nhiêu',
    '"sản phẩm" giá như thế nào',
    # '"sản phẩm" được bán bao nhiêu',

    # Mô tả
    '"sản phẩm" được mô tả như thế nào',

    # Công thức
    '"sản phẩm" được chế biến như thế nào',
    'công thức của "sản phẩm" là gì',

    # Loại sản phẩm
    'loại sản phẩm của "sản phẩm" là gì',
]

sizeWords = [
    'kích cỡ',
    'size',
    'kích thước',
    'cỡ'
]

sizes = [
    'nhỏ',
    'vừa',
    'lớn'
]

resultInformationQuestions = []

for question in productInformationQuestions:
  for begin0 in begin0s:
    for begin1 in begin1s:
      for begin2 in begin2s:
        for separa in separas:
          for end in ends:
            for productName in productNames:

              # Về kích thước
              # if question == '"sản phẩm" còn hàng không' or question == 'hiện tại có thể có bao nhiêu "sản phẩm"' or question == 'hiện tại có thể đặt bao nhiêu "sản phẩm"':
              #   for sizeWord in sizeWords:
              #      for size in sizes:
                    # questionChange = question.replace('"sản phẩm"', productName + " với " + sizeWord + " " + size)
                    # newQuestionChange = begin0 + ("" if begin0 == "" else separa) + begin1 + ("" if begin1 == "" else separa) + begin2 + ("" if begin2 == "" else separa) + questionChange + end

                    # resultInformationQuestions.append(newQuestionChange)

                    # questionChange = question.replace('"sản phẩm"', productName + " " + sizeWord + " " + size)
                    # newQuestionChange = begin0 + ("" if begin0 == "" else separa) + begin1 + ("" if begin1 == "" else separa) + begin2 + ("" if begin2 == "" else separa) + questionChange + end

                    # resultInformationQuestions.append(newQuestionChange)

              # Thay thế tên sản phẩm
              quenstionReplace = question.replace('"sản phẩm"', productName)

              newQuestion = begin0 + ("" if begin0 == "" else separa) + begin1 + ("" if begin1 == "" else separa) + begin2 + ("" if begin2 == "" else separa) + quenstionReplace + end

              resultInformationQuestions.append(newQuestion)


resultInformationQuestions = list(set(resultInformationQuestions))

print("Số lượng dữ liệu của câu hỏi về thông tin sản phẩm:", len(resultInformationQuestions))

# Hiển thị câu hỏi
# resultInformationQuestions

productTopSellQuestions = [
    'danh sách sản phẩm bán chạy hàng đầu',
    'danh sách sản phẩm bán chạy nhất',
    'danh sách sản phẩm được mua nhiều nhất'
    'danh sách sản phẩm được mua hàng đầu',
    'danh sách sản phẩm được mọi người mua nhiều nhất',
    'danh sách sản phẩm được mọi người mua hàng đầu',
    'danh sách sản phẩm thu hút mọi người nhất',
    'danh sách sản phẩm thu hút mọi người mua nhièu nhất',
]

resultProductTopSellQuestions = []

for question in productTopSellQuestions:
  for begin0 in begin0s:
    for begin1 in begin1s:
      for begin2 in begin2s:
        for separa in separas:
          for end in ends:
            newQuestion = begin0 + ("" if begin0 == "" else separa) + begin1 + ("" if begin1 == "" else separa) + begin2 + ("" if begin2 == "" else separa) + question + end

            resultProductTopSellQuestions.append(newQuestion)

resultProductTopSellQuestions = list(set(resultProductTopSellQuestions))

print("Số lượng dữ liệu của câu hỏi về sản phẩm bán chạy:", len(resultProductTopSellQuestions))

# Hiển thị câu hỏi
# resultProductTopSellQuestions

recommendedProductQuestions = [
    'gợi ý sản phẩm',
    'gợi ý cà phê',
    'gợi ý sản phẩm cho "object"',
    'gợi ý sản phẩm cho "object" với',
    'gợi ý sản phẩm cho "object" được không',
    'gợi ý sản phẩm cho "object" đi',
    'gợi ý cà phê tương tự cho "object" với',
    'gợi ý cà phê tương tự cho "object" được không',
    'gợi ý cà phê tương tự cho "object" đi',
    'gợi ý cho "object" cà phê tương tự với',
    'gợi ý cho "object" cà phê tương tự được không',
    'gợi ý cho "object" cà phê tương tự đi',
    'gợi ý cho "object" sản phẩm với',
    'gợi ý cho "object" sản phẩm được không',
    'gợi ý cho "object" sản phẩm đi',
    'cho "object" hỏi sản phẩm tương tự với',
    'cho "object" hỏi sản phẩm tương tự được không',
    'cho "object" hỏi sản phẩm tương tự đi',
    'cho "object" hỏi cà phê tương tự với',
    'cho "object" hỏi cà phê tương tự được không',
    'cho "object" hỏi cà phê tương tự đi',
    'sản phẩm tương tự',
    'cà phê tương tự',
]

resultRecommendedProductQuestions = []

for question in recommendedProductQuestions:
  for begin0 in begin0s:
    for begin1 in begin1s:
      for separa in separas:
        for end in ends:
          for objectSelect in objects:
            newQuestion = begin0 + ("" if begin0 == "" else separa) + begin1 + ("" if begin1 == "" else separa) + question + end

            newQuestion = newQuestion.replace('"object"', objectSelect)

            resultRecommendedProductQuestions.append(newQuestion)

resultRecommendedProductQuestions = list(set(resultRecommendedProductQuestions))

print("Số lượng dữ liệu của câu hỏi về gợi ý sản phẩm nhanh chóng:", len(resultRecommendedProductQuestions))

# Hiển thị câu hỏi
# resultRecommendedProductQuestions

"""## Xuất file excel"""

import pandas as pd

sizeInformationQuestion = len(resultInformationQuestions)
sizeProductTopSellQuestion = len(resultProductTopSellQuestions)
sizeRecommendedProductQuestion = len(resultRecommendedProductQuestions)

questions = resultInformationQuestions + resultProductTopSellQuestions + resultRecommendedProductQuestions

print("Số lượng dữ liệu câu hỏi: ", len(questions))

infomations = [1] * sizeInformationQuestion + [0] * sizeProductTopSellQuestion + [0] * sizeRecommendedProductQuestion
productTopSells = [0] * sizeInformationQuestion + [1] * sizeProductTopSellQuestion + [0] * sizeRecommendedProductQuestion
recommendedProducts = [0] * sizeInformationQuestion + [0] * sizeProductTopSellQuestion + [1] * sizeRecommendedProductQuestion

# Tạo DataFrame từ dữ liệu
data = {'Question': questions,
        'infomation': infomations,
        'productTopSell': productTopSells,
        'recommendedProduct': recommendedProducts}

df = pd.DataFrame(data)
df = df.sample(frac=1).reset_index(drop=True)

import os

# Đảm bảo thư mục tồn tại trước khi ghi vào file Excel
directory = '/content/drive/MyDrive/Datatrain/coffee/'
if not os.path.exists(directory):
    os.makedirs(directory)

# Ghi DataFrame vào file Excel
df.to_csv('/content/drive/MyDrive/Datatrain/coffee/coffee.csv', index=False)

print("Xuất ra thành công!")

"""# 0.2. Data augmentation"""

import os

path = "/content/drive/My Drive/BERT"
os.chdir(path)
!ls

!pip3 install fairseq
!pip3 install fastbpe
!pip3 install vncorenlp
!pip3 install transformers

#Download VnCoreNLP-1.1.1.jar & its word segmentation component (i.e. RDRSegmenter)
!mkdir -p vncorenlp/models/wordsegmenter
!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar
!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab
!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr
!mv VnCoreNLP-1.1.1.jar vncorenlp/
!mv vi-vocab vncorenlp/models/wordsegmenter/
!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/

!wget https://public.vinai.io/PhoBERT_base_fairseq.tar.gz
!tar -xzvf PhoBERT_base_fairseq.tar.gz

from fairseq.models.roberta import RobertaModel
phoBERT = RobertaModel.from_pretrained('PhoBERT_base_fairseq', checkpoint_file='model.pt')
phoBERT.eval()  # disable dropout (or leave in train mode to finetune

from vncorenlp import VnCoreNLP
rdrsegmenter = VnCoreNLP("vncorenlp/VnCoreNLP-1.1.1.jar", annotators="wseg", max_heap_size='-Xmx500m')

text = 'Tôn Ngộ Không phò Đường Tăng đi thỉnh kinh tại Tây Trúc'
text_masked = 'Học sinh được  <mask> do dịch covid-19'
# Tokenize câu gốc và thay từ phò bằng <mask>
words = rdrsegmenter.tokenize(text)[0]
for i, token in enumerate(words):
  if token == 'phò':
    words[i] = ' <mask>'
text_masked_tok = ' '.join(words)
print('text_masked_tok: \n', text_masked_tok)

ques = 'cậu cho em hỏi loại sản phẩm của cà phê  <mask> là gì'

from fairseq.data.encoders.fastbpe import fastBPE
from fairseq import options
import numpy as np

# Khởi tạo Byte Pair Encoding cho PhoBERT
class BPE():
  bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'
args = BPE()
phoBERT.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT

# Filling marks
topk_filled_outputs = phoBERT.fill_mask(ques, topk=10)
topk_probs = [item[1] for item in topk_filled_outputs]
print('Total probability: ', np.sum(topk_probs))
print('Input sequence: ', ques)
print('Top 10 in mask: ')
for i, output in enumerate(topk_filled_outputs):
  print(output[0])

"""# I. Preprocessing data"""

import pandas as pd

train_file = pd.read_csv('/content/coffee.csv', encoding='utf-8')

import pandas as pd

train_file = pd.read_csv('/content/drive/MyDrive/Datatrain/coffee/coffee.csv', encoding='utf-8')

train_file

train_contexts = train_file['Question'].tolist()

train_contexts

len(train_contexts)

train_labels = train_file.drop(columns=['Question'])

train_labels

train_labels = train_labels.to_numpy()

train_labels

len(train_labels)

"""# II. Model"""

!pip install tensorflow_text
!pip install tensorflow-hub
!pip install transformers==4.3.0
!pip install sentencepiece
!pip install -q underthesea
!pip install -q sentence_transformers
!pip install py_vncorenlp

import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModel
from tensorflow.keras.models import Sequential, Model, load_model
from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, Dense, Dropout, Input, Lambda, Concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

"""## Input model"""

bert_name = "vinai/phobert-base"

# Load the pre-trained sentence embedding model
bert = TFAutoModel.from_pretrained(bert_name)
tokenizer = AutoTokenizer.from_pretrained(bert_name)

seq_len = [len(i.split()) for i in train_contexts]
pd.Series(seq_len).hist(bins = 30)

MAX_LEN_CONTEXT = 23

attention_mask_context = Input(shape=(MAX_LEN_CONTEXT,), dtype=tf.int32, name='attention_mask_context')
input_ids_context = Input(shape=(MAX_LEN_CONTEXT,), dtype=tf.int32, name='input_ids_context')

# Apply the bert embedding function using a Lambda layer
embedding_context = Lambda(lambda inputs: bert(input_ids=inputs[0], attention_mask=inputs[1])[0])(
                        [input_ids_context, attention_mask_context])

embedding_context

# TRAIN ENCODING

train_context_encodings = tokenizer(train_contexts, truncation=True, padding='max_length', max_length=MAX_LEN_CONTEXT)

train_context_features = {key: tf.convert_to_tensor(train_context_encodings[key], dtype=tf.float32) for key in tokenizer.model_input_names}

train_context_features.keys()

train_context_input_ids = train_context_features['input_ids']
train_context_attention_mask = train_context_features['attention_mask']

"""## Label decode"""

def labelDecoder(number):
  if number == 2:
    return 'Recommend'
  elif number == 1:
    return 'Top sell'
  elif number == 0:
    return 'Information'
  else:
    return 'Other'

"""## Model (CNN)"""

# CNN layer.
cnn_layer = tf.keras.layers.Conv1D(
    filters=100,
    kernel_size=4,
    # Use 'same' padding so outputs have the same shape as inputs.
    padding='same')

# Query encoding of shape [batch_size, Tq, filters].
context_seq_encoding = cnn_layer(embedding_context)

# Reduce over the sequence axis to produce encodings of shape
# [batch_size, filters].
context_encoding = tf.keras.layers.GlobalAveragePooling1D()(context_seq_encoding)

# Concatenate query and document encodings to produce a DNN input layer.
input_layer = tf.keras.layers.Concatenate()(
    [context_encoding])

x = Dropout(0.5)(input_layer)
out = Dense(3, activation='softmax')(x)

focal_loss = tf.keras.losses.CategoricalFocalCrossentropy(alpha=0.25, gamma=2.0)

checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)
#Model
model = Model(
    inputs=[attention_mask_context, input_ids_context],
    outputs=out,
)

model.compile(optimizer=Adam(learning_rate=0.00001),
              loss=focal_loss,
              metrics=['categorical_accuracy'])

# In thông tin về kiến trúc mô hình
model.summary()

"""## Model Sequential"""

cnn_model = Sequential()
cnn_model.add(Conv1D(128, kernel_size=5, padding='same', input_shape=(MAX_LEN_CONTEXT, embedding_context.shape[-1])))
cnn_model.add(Conv1D(64, kernel_size=3, activation='relu'))
cnn_model.add(GlobalAveragePooling1D())
cnn_model.add(Dense(32, activation='relu'))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(train_labels.shape[1], activation='softmax'))

# Apply Sequential model to embeddings
output = cnn_model(embedding_context)

# Define full model
model = Model(inputs=[input_ids_context, attention_mask_context], outputs=output)

# Compile model
model.compile(optimizer=Adam(learning_rate=0.00001),
              loss='categorical_crossentropy',
              metrics=['categorical_accuracy'])

# Print model summary
model.summary()

"""## Train"""

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

model.fit([train_context_input_ids, train_context_attention_mask],
          train_labels,
          batch_size = 40,
          epochs = 10,
          validation_split=0.2,
          callbacks=[early_stopping])

"""## Predict"""

test_contexts = ["cà phê còn hàng không",
                 "gợi ý các cà phê tương tự với",
                 "cửa hàng gợi ý sản phẩm cho tớ được không",
                 "cho tôi các sản phẩm bán chạy nhất",
                 "danh sách sản phẩm bán chạy hàng đầu",
                 "cà phê quán này ngon không",
                 "Espresso với americano cái nào ngon hơn",
                 "cho hỏi thông tin về Espresso",
                 "Espresso còn bao nhiêu ly",
                 "Espresso giá như nào"]
test_context_encodings = tokenizer(test_contexts, truncation=True, padding='max_length', max_length=MAX_LEN_CONTEXT)

test_context_features = {key: tf.convert_to_tensor(test_context_encodings[key], dtype=tf.float32) for key in tokenizer.model_input_names}

test_context_features.keys()

test_context_input_ids = test_context_features['input_ids']
test_context_attention_mask = test_context_features['attention_mask']

Test_array = model.predict([test_context_input_ids, test_context_attention_mask])

Test_array

y_predict = np.argmax(Test_array, axis=-1)

y_predict

y_predict_decoder = []

for predict in y_predict:
  y_predict_decoder.append(labelDecoder(predict))

y_predict_decoder

"""## Save model"""

model.save('model_coffee_question_classification.h5')

model = load_model('model_coffee_question_classification.h5')